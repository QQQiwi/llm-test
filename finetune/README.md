# Содержание

Данные, которые были получены вследствие парсинга сайтов и ресерча на Kaggle,
были предобработаны и сохранены в качестве выборки для обучения.

На данный момент доступны:

* [x] llama2-7b
* [x] unsloth/llama-2-7b-chat-bnb-4bit (10 эпох, длина последовательности 2048)
* [x] unsloth/llama-2-7b-chat-bnb-4bit (40 эпох, длина последовательности 2048)
* [x] unsloth/llama-2-7b-chat-bnb-4bit (40 эпох, длина последовательности 300)

## Параметры обучения unsloth/llama-2-7b-chat-bnb-4bit

Было применен модифицированный Low-Rank Adaptation с квантованием до 4-х бит.
Также использовался RoPE scaling, для увеличения длины генерируемой последовательности.

* lora_alpha = 16
* lora_dropout = 0
* lora_r = 16
* gradient_accumulation_steps = 4
* оптимизатор: "adamw_8bit"
* количество эпох: 60
* weight_decay = 0.01

Затем применялось 8-битное квантование и веса модели сохранялись в формат
**gguf**.

Для каждой модели был создан Modelfile, с помощью него и весов в формате **gguf**
были инициализированы LLM модели, доступные через интерфейс Ollama.
